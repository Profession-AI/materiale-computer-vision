{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPJ3KGqQIu86kfoydNfhXql"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Neural Net per XOR**\n","\n","L'operazione di `XOR` è un'operazione bitwise non lineare.\n","\n","| x1 | x2 | x1 XOR x2 |\n","|----|----|-----------|\n","| 0  | 0  | 0         |\n","| 0  | 1  | 1         |\n","| 1  | 0  | 1         |\n","| 1  | 1  | 0         |\n","\n","Una neural net è adatta ad imparare questo tipo di task data la non linearità del problema\n"],"metadata":{"id":"x5mfjNsDzMl6"}},{"cell_type":"markdown","source":["## **Dataset**"],"metadata":{"id":"NWxo8S3P2mq5"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"umVEo2CQ3DU-","executionInfo":{"status":"ok","timestamp":1715438711389,"user_tz":-120,"elapsed":5321,"user":{"displayName":"Riccardo Musmeci","userId":"06138046899083233740"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["x_train_tensor = torch.tensor(\n","    [[0,0],[0,1],[1,0],[1,1]]\n",").float()\n","\n","y_train_tensor = torch.tensor(\n","    [0,1,1,0]\n",").view(4,1).float()\n","\n","x_train_tensor.shape, y_train_tensor.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gOibC4W62rP4","executionInfo":{"status":"ok","timestamp":1715438711389,"user_tz":-120,"elapsed":4,"user":{"displayName":"Riccardo Musmeci","userId":"06138046899083233740"}},"outputId":"7719151e-ea49-40d6-fed4-328deeb43518"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([4, 2]), torch.Size([4, 1]))"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["x_val_tensor = torch.clone(x_train_tensor)\n","y_val_tensor = torch.clone(y_train_tensor)\n","\n","x_val_tensor.shape, y_val_tensor.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rctGCNeW20aC","executionInfo":{"status":"ok","timestamp":1715438711389,"user_tz":-120,"elapsed":2,"user":{"displayName":"Riccardo Musmeci","userId":"06138046899083233740"}},"outputId":"7dd098c5-c37b-4912-8933-b431c12087c6"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([4, 2]), torch.Size([4, 1]))"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["## **Neural Net con PyTorch**\n","Il modulo `nn` contiene tutte le operazioni e i tools per addestrare reti neurali di qualsiasi dimensione e per qualsiasi task\n","\n","* `nn.Module` --> classe utile alla definizione della rete neurale e alle loss. La classe contiene i pesi della rete, mantiene i gradienti ed esegue la backpropagation\n","* `optim` --> sotto-module che contiene una vasta scelta di optimizer (e.g. SGD, Adam, etc)\n","\n"],"metadata":{"id":"7_J73jusyNeO"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"uPE5ZSpZt42m","executionInfo":{"status":"ok","timestamp":1715438711389,"user_tz":-120,"elapsed":2,"user":{"displayName":"Riccardo Musmeci","userId":"06138046899083233740"}}},"outputs":[],"source":["import torch.nn as nn\n","import torch.optim as optim"]},{"cell_type":"markdown","source":["Definizione rete neurale (MLP):\n","* input: N esempi di dimensione 2 ([x1, x2]) - dim: Nx2\n","* hidden layers: un Linear layer con 2 neuroni in input e 2 neuroni in output -- dim: 2x2\n","* output layer: un Linear layer con 2 neuroni in input e 1 in output -- dim: 2x1"],"metadata":{"id":"FEY-eas21TsJ"}},{"cell_type":"code","source":["class Net(nn.Module):\n","\n","  def __init__(self, input_dim: int = 2, output_dim: int = 1) -> None:\n","\n","    super().__init__()\n","    hidden_out = 2\n","    self.hidden = nn.Linear(\n","        in_features=input_dim, out_features=hidden_out\n","    )\n","    self.output = nn.Linear(\n","        in_features=hidden_out, out_features=output_dim\n","    )\n","    self.activation = nn.Sigmoid()\n","\n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","    # x shape --> (N, 2)\n","    #   - N --> batch size\n","    #   - 2 --> [x1, x2]\n","    x = self.hidden(x) # [Nx2]\n","    x = self.activation(x)\n","    logits = self.output(x) # [Nx1]\n","    return logits"],"metadata":{"id":"asTAySfXyL8H","executionInfo":{"status":"ok","timestamp":1715438711389,"user_tz":-120,"elapsed":1,"user":{"displayName":"Riccardo Musmeci","userId":"06138046899083233740"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["model = Net(2, 1)"],"metadata":{"id":"KQBSiVbZ0RI-","executionInfo":{"status":"ok","timestamp":1715438711389,"user_tz":-120,"elapsed":1,"user":{"displayName":"Riccardo Musmeci","userId":"06138046899083233740"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Proviamo se Net funziona correttamente con diversi tipi e dimensioni di input"],"metadata":{"id":"nZd5B6Z03t_N"}},{"cell_type":"code","source":["batch_size = 2\n","x = torch.rand(size=(batch_size, 2)) # e.g. [[0.5, 0.2]]\n","print(\"Input data info:\")\n","print(f\"\\t- input dim: {x.shape}\")\n","print(f\"\\t- input data: \\n{x}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sqXOU66T3r6f","executionInfo":{"status":"ok","timestamp":1715438711984,"user_tz":-120,"elapsed":6,"user":{"displayName":"Riccardo Musmeci","userId":"06138046899083233740"}},"outputId":"af40917c-5cb9-4946-d52c-72bee806c08b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Input data info:\n","\t- input dim: torch.Size([2, 2])\n","\t- input data: \n","tensor([[0.0533, 0.8607],\n","        [0.0653, 0.5055]])\n"]}]},{"cell_type":"code","source":["# forward pass\n","logits = model(x)\n","print(\"Logits data info:\")\n","print(f\"\\t- logits dim: {logits.shape}\")\n","print(f\"\\t- input data: \\n{logits}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j0JXib_V34sV","executionInfo":{"status":"ok","timestamp":1715438711984,"user_tz":-120,"elapsed":4,"user":{"displayName":"Riccardo Musmeci","userId":"06138046899083233740"}},"outputId":"e5dcda36-b227-43da-cb71-6c1476bb344e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Logits data info:\n","\t- logits dim: torch.Size([2, 1])\n","\t- input data: \n","tensor([[-0.7958],\n","        [-0.8132]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["Proviamo sui dati di train e val dello XOR"],"metadata":{"id":"BCkgpXy26rhz"}},{"cell_type":"code","source":["print(\"Input data info:\")\n","print(f\"\\t- input dim: {x_train_tensor.shape}\")\n","print(f\"\\t- input data: \\n{x_train_tensor}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LIx3KM6P6wjG","executionInfo":{"status":"ok","timestamp":1715438711984,"user_tz":-120,"elapsed":2,"user":{"displayName":"Riccardo Musmeci","userId":"06138046899083233740"}},"outputId":"739a9d25-33dc-469a-e0e4-1546ce5546b5"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Input data info:\n","\t- input dim: torch.Size([4, 2])\n","\t- input data: \n","tensor([[0., 0.],\n","        [0., 1.],\n","        [1., 0.],\n","        [1., 1.]])\n"]}]},{"cell_type":"code","source":["# forward pass\n","logits = model(x_train_tensor)\n","print(\"Logits data info:\")\n","print(f\"\\t- logits dim: {logits.shape}\")\n","print(f\"\\t- input data: \\n{logits}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xJpD7h-D7F1o","executionInfo":{"status":"ok","timestamp":1715438712388,"user_tz":-120,"elapsed":405,"user":{"displayName":"Riccardo Musmeci","userId":"06138046899083233740"}},"outputId":"f1a044f0-51aa-435a-c9cc-e59f1a7ea692"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Logits data info:\n","\t- logits dim: torch.Size([4, 1])\n","\t- input data: \n","tensor([[-0.8362],\n","        [-0.7877],\n","        [-0.8664],\n","        [-0.8168]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["### **Training Setup**\n","\n","1) Optimizer: useremo la [Stochastic Gradient Descent](https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31) con learning rate a 0.01\n","\n","2) Loss: Mean Squared Error: MSE = $\\frac{1}{n} \\Sigma_{i=1}^n({y}-\\hat{y})^2$"],"metadata":{"id":"RVMMj4rM5HDG"}},{"cell_type":"code","source":["model = Net(2, 1)\n","learning_rate = 0.01\n","optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n","criterion = nn.MSELoss(reduction=\"mean\")"],"metadata":{"id":"UnLY9CzH4IFA","executionInfo":{"status":"ok","timestamp":1715438719402,"user_tz":-120,"elapsed":7015,"user":{"displayName":"Riccardo Musmeci","userId":"06138046899083233740"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### **Training Loop**"],"metadata":{"id":"KmlEAAEf6R4U"}},{"cell_type":"code","source":["model.train()\n","for epoch in range(1):\n","  # gradienti di ogni peso in ogni layer a zero\n","  optimizer.zero_grad()\n","  # Check dei gradienti\n","  print(f\"Check gradienti dopo zero_grad:\")\n","  print(f\"\\t- hidden layer: {model.hidden.weight.grad}\")\n","  print(f\"\\t- output layer: {model.output.weight.grad}\")\n","  # forward pass\n","  logits = model(x_train_tensor)\n","  print(f\"Logits: {logits}\")\n","  # loss computation\n","  loss = criterion(logits, y_train_tensor)\n","  print(f\"Loss: {loss:.4f}\")\n","  # calcolo dei gradienti -- backprop\n","  loss.backward()\n","  # Check dei gradienti\n","  print(f\"Check gradienti dopo backpropagation:\")\n","  print(f\"\\t- hidden layer: {model.hidden.weight.grad}\")\n","  print(f\"\\t- output layer: {model.output.weight.grad}\")\n","  # step dell'optimizer\n","  optimizer.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H2iyzgch6Z4J","executionInfo":{"status":"ok","timestamp":1715438719402,"user_tz":-120,"elapsed":4,"user":{"displayName":"Riccardo Musmeci","userId":"06138046899083233740"}},"outputId":"fedfdf9c-0bfd-46de-fe08-977d98f1dc39"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Check gradienti dopo zero_grad:\n","\t- hidden layer: None\n","\t- output layer: None\n","Logits: tensor([[-0.3461],\n","        [-0.2486],\n","        [-0.3004],\n","        [-0.2030]], grad_fn=<AddmmBackward0>)\n","Loss: 0.8527\n","Check gradienti dopo backpropagation:\n","\t- hidden layer: tensor([[-0.1160, -0.1118],\n","        [ 0.0230,  0.0221]])\n","\t- output layer: tensor([[-0.7678, -0.7830]])\n"]}]},{"cell_type":"code","source":["# ripartiamo da random weights\n","model = Net(2, 1)\n","learning_rate = 0.01\n","optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n","criterion = nn.MSELoss(reduction=\"mean\")\n","n_epochs = 40000"],"metadata":{"id":"_7Cek4Na9Q-T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(n_epochs):\n","  model.train()\n","  # forward pass\n","  logits = model(x_train_tensor)\n","  # loss computation\n","  loss = criterion(logits, y_train_tensor)\n","  # calcolo dei gradienti -- backprop\n","  loss.backward()\n","  # step dell'optimizer\n","  optimizer.step()\n","  # gradienti di ogni peso in ogni layer a zero\n","  optimizer.zero_grad()\n","  # log loss + validation\n","  if (epoch % int(0.05*n_epochs)) == 0:\n","    # validation\n","    model.eval()\n","    y_preds = model(x_val_tensor)\n","    val_loss = criterion(y_preds, y_val_tensor)\n","    print(f\"epoch [{epoch}/{n_epochs}], train_loss: {loss:.3f} - val_loss: {val_loss:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"no238_ky7vTJ","executionInfo":{"status":"ok","timestamp":1715421924045,"user_tz":-120,"elapsed":26011,"user":{"displayName":"Riccardo Musmeci","userId":"06138046899083233740"}},"outputId":"4e2ad423-ba3c-4a38-b794-d25945062126"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch [0/40000], train_loss: 0.669 - val_loss: 0.646\n","epoch [2000/40000], train_loss: 0.251 - val_loss: 0.251\n","epoch [4000/40000], train_loss: 0.251 - val_loss: 0.251\n","epoch [6000/40000], train_loss: 0.250 - val_loss: 0.250\n","epoch [8000/40000], train_loss: 0.250 - val_loss: 0.250\n","epoch [10000/40000], train_loss: 0.250 - val_loss: 0.250\n","epoch [12000/40000], train_loss: 0.250 - val_loss: 0.250\n","epoch [14000/40000], train_loss: 0.250 - val_loss: 0.250\n","epoch [16000/40000], train_loss: 0.250 - val_loss: 0.250\n","epoch [18000/40000], train_loss: 0.249 - val_loss: 0.249\n","epoch [20000/40000], train_loss: 0.249 - val_loss: 0.249\n","epoch [22000/40000], train_loss: 0.248 - val_loss: 0.248\n","epoch [24000/40000], train_loss: 0.247 - val_loss: 0.247\n","epoch [26000/40000], train_loss: 0.245 - val_loss: 0.245\n","epoch [28000/40000], train_loss: 0.239 - val_loss: 0.239\n","epoch [30000/40000], train_loss: 0.227 - val_loss: 0.227\n","epoch [32000/40000], train_loss: 0.202 - val_loss: 0.202\n","epoch [34000/40000], train_loss: 0.161 - val_loss: 0.161\n","epoch [36000/40000], train_loss: 0.104 - val_loss: 0.104\n","epoch [38000/40000], train_loss: 0.045 - val_loss: 0.045\n"]}]},{"cell_type":"markdown","source":["### Inference"],"metadata":{"id":"GMtaykuxBZHh"}},{"cell_type":"code","source":["model.train()\n","print(model.training)\n","model.eval()\n","print(model.training)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rFYJbbry79TK","executionInfo":{"status":"ok","timestamp":1715421934149,"user_tz":-120,"elapsed":3,"user":{"displayName":"Riccardo Musmeci","userId":"06138046899083233740"}},"outputId":"08771733-98ba-4b72-b656-291752f6ad95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","False\n"]}]},{"cell_type":"code","source":["input_data = [1., 1.] # target 1\n","x = torch.tensor([input_data], requires_grad=False)\n","logit = model(x)\n","y_pred = 1 if logit[0] > 0.5 else 0\n","print(f\"Logit: {logit}\")\n","print(f\"[x1, x2] = [{int(input_data[0])}, {int(input_data[1])}]\")\n","print(f\"x1 XOR x2: {y_pred}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Avxv4RsTHedF","executionInfo":{"status":"ok","timestamp":1715438754296,"user_tz":-120,"elapsed":2,"user":{"displayName":"Riccardo Musmeci","userId":"06138046899083233740"}},"outputId":"4a48c8eb-55e7-405c-cb1b-a5e69162c137"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Logit: tensor([[-0.1783]], grad_fn=<AddmmBackward0>)\n","[x1, x2] = [1, 1]\n","x1 XOR x2: 0\n"]}]},{"cell_type":"code","source":["with torch.no_grad():\n","  input_data = [1., 1.] # target 1\n","  x = torch.tensor([input_data], requires_grad=False)\n","  logit = model(x)\n","  y_pred = 1 if logit[0] > 0.5 else 0\n","  print(f\"Logit: {logit.item():.4f}\")\n","  print(f\"[x1, x2] = [{int(input_data[0])}, {int(input_data[1])}]\")\n","  print(f\"x1 XOR x2: {y_pred}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kuzpu7j7Bhbt","executionInfo":{"status":"ok","timestamp":1715438738572,"user_tz":-120,"elapsed":2,"user":{"displayName":"Riccardo Musmeci","userId":"06138046899083233740"}},"outputId":"36678469-b3c6-4186-9a19-ed8be4178c2a"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Logit: -0.1783\n","[x1, x2] = [1, 1]\n","x1 XOR x2: 0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"mdtETueNB69D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"auHguIe5CANL"},"execution_count":null,"outputs":[]}]}